---
permalink: /
layout: page
title: Welcome
---

Hi, I am Alex Falcon. I am a Post-doc researcher at the [University of Udine](https://www.uniud.it/it) ([AI Lab](http://ailab.uniud.it/)). I completed my PhD in Computer Science, Mathematics and Physics, jointly held at [Fondazione Bruno Kessler](https://www.fbk.eu/it/) ([Technology of Vision - TeV](https://tev.fbk.eu/)) and University of Udine, under the supervision of [Oswald Lanz](https://www.unibz.it/it/faculties/engineering/academic-staff/person/46208-oswald-lanz) (Free University of Bolzano) and [Giuseppe Serra](https://people.uniud.it/page/giuseppe.serra) (University of Udine). My main research focus is currently focused towards multimedia, video and language understanding, and deep learning. Before that, I completed my Bachelor's and Master's degree in Computer Science at the University of Udine. Specifically, during my Master's I started working with AI, machine learning, and deep learning with a focus on Predictive Maintenance.

E-mail: falcon.alex 'at' spes.uniud.it / [Google Scholar](https://scholar.google.com/citations?user=sHPhexYAAAAJ&hl=it) / [Github](https://github.com/aranciokov) / [LinkedIn](https://www.linkedin.com/in/alex-falcon-9b1a231a3) / [CV](https://github.com/aranciokov/aranciokov.github.io/blob/master/CV-1.pdf)

<h3>News <span class="fa-solid fa-bullhorn"></span></h3>
<ul>
  <li>We are organizing a challenge on <a href="https://ailab.uniud.it/apartment-recommendation-challenge/">Metaverse Apartment Retrieval Challenge</a>!</li>
  <li>I am part of the local organization committee for the <a href="http://eqai.eu/">3rd edition of the European Summer School on Quantum AI</a>!</li>
  <li>I am a guest editor for the <b>Special Issue on Text-Multimedia Retrieval: Retrieving Multimedia Data by Means of Natural Language</b> at <b>ACM TOMM</b>! Check the <a href="https://dl.acm.org/pb-assets/static_journal_pages/tomm/pdf/ACM_SI_Text_Multimedia_Retrieval-1708635324153.pdf">call for papers</a>! (deadline: June 30, 2024)</li>
  <li>one paper accepted as an Oral at <a href="https://ceur-ws.org/Vol-3643/paper17.pdf">IRCDL 2024</a>!</li>
 </ul>
<details>
<summary>[click for previous years]</summary>
<ul>
 <li><em>2023</em></li>
 <ul>
  <li>one paper accepted as an Oral at <a href="https://link.springer.com/chapter/10.1007/978-3-031-53311-2_35"><b>MMM 2024</b></a>! <a href="https://github.com/aliabdari/NLP_to_rank_artistic_Metaverses">code by Ali</a></li>
  <li>one paper accepted at <a href="https://dl.acm.org/doi/abs/10.1145/3606040.3617445">MMIR@<b>ACM MM 2023</b></a> and one paper accepted at <a href="https://openaccess.thecvf.com/content/ICCV2023W/CV4Metaverse/html/Abdari_FArMARe_a_Furniture-Aware_Multi-Task_Methodology_for_Recommending_Apartments_Based_on_ICCVW_2023_paper.html">CV4Metaverse@<b>ICCV 2023</b></a>! Codebases by Ali: <a href="https://github.com/aliabdari/Metaverse_Retrieval">code1</a> <a href="https://github.com/aliabdari/FArMARe">code2</a></li>
  <li>I had a great experience at the <a href="https://www.ellis.unimore.it/summer-school/"><b>ELLIS Summer School on Large-Scale AI for Research and Industry</b></a> in Modena, Italy!</li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-031-43153-1_16">one paper</a> accepted at ICIAP 2023!</li>
  <li><a href="https://arxiv.org/abs/2306.15445">our solution (report)</a>, trained with only 25% of the data, got <b>3rd place</b> in the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge @ CVPR 2023!</li>
  <li>I am part of the local organization committee for <a href="https://iciap2023.org/">ICIAP 2023</a>!</li>
  <li>I delivered a seminar on "Deep Learning for Multimedia understanding" as the speaker at University of Udine!</li>
  <li>I am part of the local organization committee for the <a href="http://eqai.eu/"><b>2nd edition of the European Summer School on Quantum AI</b></a>!</li>
  <li>March, 13th 2023: I successfully defended my <a href="https://air.uniud.it/handle/11390/1252364"><b>PhD thesis</b></a> <em>cum laude</em>!</li>
  <li>[one paper](https://link.springer.com/article/10.1007/s11042-023-14333-0) accepted at <b>Multimedia Tools and Applications</b>! <a href="https://github.com/aranciokov/MT-VideoQA">code</a></li>
 </ul>
 <li><em>2022</em></li>
 <ul>
  <li><a href="https://ceur-ws.org/Vol-3463/paper2.pdf">one paper (pdf)</a> accepted as an Oral at AIABI@AIxIA 2022!</li>
  <li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0166361522001592">one paper</a> accepted at <b>Computers in Industry</b>! <a href="https://github.com/aranciokov/NTM-For-RULEstimation">code</a></li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548365">one paper</a> accepted as an Oral at <b>ACM MM 2022</b>! <a href="https://github.com/aranciokov/FSMMDA_VideoRetrieval">code</a></li>
  <li>I delivered two talks at University of Bolzano: "Data-driven approaches for the Remaining Useful Life Estimation problem" and "Learning video retrieval models with relevance-aware online mining"</li>
  <li>I was featured in the <a href="https://magazine.fbk.eu/it/news/un-riconoscimento-internazionale-per-la-comprensione-semantica-di-video/">FBK magazine</a> (<em>italian</em>)!</li>
  <li><a href="https://arxiv.org/abs/2206.10903">our solution (report)</a> got <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/trophy.png" alt="(trophy emoji)" width="16px" height="auto"> <b>1st place</b> <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/trophy.png" alt="(trophy emoji)" width="16px" height="auto"> in the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge @ CVPR 2022!</li>
  <li>I attended the fantastic <a href="https://iplab.dmi.unict.it/icvss2022/"><b>International Computer Vision Summer School (ICVSS)</b></a> in Scicli, Italy and presented a <a href="https://github.com/aranciokov/aranciokov.github.io/blob/master/poster_ICVSS-1.pdf">poster</a> titled "Relevance-aware Online Mining for Video Retrieval"!</li>
  <li><a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531395">one paper</a> accepted as an Oral at <b>ICMR 2022</b>! <a href="https://github.com/aranciokov/RelevanceMargin-ICMR22">code</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-031-06433-3_16">one paper</a> accepted as an Oral at ICIAP 2021! <a href="https://github.com/aranciokov/ranp">code</a></li>
  <li>I delivered a seminar on "Data-driven approaches for the remaining useful life estimation problem" as the speaker at FBK!</li>
  </ul>
<li><em>2021</em></li>
 <ul>
  <li><a href="https://arxiv.org/abs/2110.02902">our solution (report)</a> got 3rd place in the EPIC-Kitchens-100 Action Recognition Challenge @ CVPR 2021!</li>
  <li>I completed the "Fundamentals of Deep Learning for Multi-GPUs" course held by NVIDIA Deep Learning Institute!</li>
  <li>we organized the <a href="https://sites.google.com/view/viqa2020">VIQA</a> workshop @ ICPR 2020 (later merged into the <a href="https://sites.google.com/view/vtiur2020/">VTIUR</a> workshop)!</li>
  </ul>
 <li><em>2020</em></li>
 <ul>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-66415-2_33">one paper</a> accepted as an Oral at EPIC@ECCV 2020!</li>
  <li>I attended the "<a href="https://boracchi.faculty.polimi.it/teaching/Non-Matrix.htm">Machine Learning for non-matrix data</a>" summer school at Politecnico di Milano!</li>
  <li><a href="http://www.papers.phmsociety.org/index.php/phme/article/view/1227">one paper</a> accepted as an Oral at PHME 2020!</li>
  <li><a href="https://ieeexplore.ieee.org/document/9187043">one paper</a> accepted as an Oral at ICPHM 2020!</li>
  </ul>
 
<li><em>2019</em></li>
<ul>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-39905-4_7">one paper</a> accepted at IRCDL 2019!</li>
  <li>October 2019: I started my PhD under the supervision of Oswald Lanz and Giuseppe Serra!</li>
  <li>July 2019: I successfully completed the Master's Degree in Computer Science <em>cum laude</em>!</li>
 </ul>
 </ul>
</details>

<h2>Projects and selected publications</h2>

<h3>On Semantic Similarity in Text-Video Retrieval</h3>
<h5><em>Topics: multimedia, cross-modal understanding, vision and language</em></h5>

<h6>Selected publications/endeavors</h6>

<table border=none>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/fsmmda.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>A Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval.</em><br/><b>A. Falcon</b>, G. Serra, O. Lanz.<br/><b>ACM MM 2022.</b></p>
    <p><b>TL;DR</b>: We propose a data augmentation technique for Text-Video Retrieval. It combines latent representations of videos (and captions) if they are a good fit (e.g., both display similar actions). No impact on inference speed, and it can be applied without changes to different modalities (video, text, but also audio, etc), consistently improving the performance on single-modalities (using HGR, about +4% nDCG, +6% mAP on EPIC-Kitchens-100) and multiple modalities (about +5% nDCG, +6% mAP on EPIC-Kitchens-100).</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/fsmmda.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>UniUD-FBK-UB-UniBZ Submission to the EPIC-Kitchens-100 Multi-Instance Retrieval Challenge 2022.</em><br/><b>A. Falcon</b>, G. Serra, S. Escalera, O. Lanz.<br/><b>EPIC@CVPR 2022.</b></p>
    <p><b>TL;DR</b>: Our solution to the challenge <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/trophy.png" alt="(trophy emoji)" width="16px" height="auto"> <b>ranked 1st</b> <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/trophy.png" alt="(trophy emoji)" width="16px" height="auto">! It uses an ensemble of models trained with our techniques developed at ACM ICMR 2022 and ICIAP 2021. Notably, it achieves these results using only the available training data, avoiding the need for millions of additional egocentric video clips, as done by other competitors.</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/ICMR22.png" alt="Overview of the algorithm" width="50%" height="auto">
    </td>
    <td width=50%>
      <p><em>Relevance-based margin for contrastively-trained video retrieval models.</em><br/><b>A. Falcon</b>, S. Sudhakaran, G. Serra, S. Escalera, O. Lanz.<br/><b>ACM ICMR 2022.</b></p>
    <p><b>TL;DR</b>: We propose a different training strategy for learning text-video retrieval models. It creates a relation between a parameter of the triplet loss function --- the margin, which is usually fixed and treated as hyperparameter --- and the input video-text data, essentially defining a 'relevance-based margin'. This removes the need for tuning the margin to obtain optimal results. Results show consistent improvements across three different neural architectures and two datasets.</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/ICIAPext.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>Learning video retrieval models with relevance-aware online mining.</em><br/><b>A. Falcon</b>, G. Serra, O. Lanz.<br/><b>ICIAP 2021.</b></p>
    <p><b>TL;DR</b>: It is difficult to describe with a caption what happens in a video. Yet, standard procedures for learning text-video retrieval models assume that only the caption(s) paired to a video is valid for it. We hypothesize it limits semantics understanding. We propose a learning strategy (called RANP) which separates captions/videos into 'irrelevant' and 'relevant', varying the standard triplet loss function to take this into account. Results confirm the hypothesis, observing +23% nDCG and +8% mAP on EPIC-Kitchens-100, +6% on MSR-VTT.</p>
    </td>
  </tr>
</table>



<h3>Ranking complex 3D scenes (Metaverses, Apartments)</h3>
<h5><em>topics: multimedia, cross-modal understanding, vision and language</em></h5>

<h6>Selected publications/endeavors</h6>

<table border=none>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/MMM24_overview.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>A Language-based solution to enable Metaverse Retrieval.</em><br/>A. Abdari, <b>A. Falcon</b>, G. Serra.<br/><b>MMM 2024.</b></p>
    <p><b>TL;DR</b>: .</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/farmare.png" alt="Overview of the algorithm" width="50%" height="auto">
    </td>
    <td width=50%>
      <p><em>FArMARe: a Furniture-Aware Multi-task methodology for Recommending Apartments based on the user interests.</em><br/>A. Abdari, <b>A. Falcon</b>, G. Serra.<br/><b>CV4Metaverse@ICCV 2023.</b></p>
    <p><b>TL;DR</b>: </p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/mmir.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>Metaverse Retrieval: Finding the Best Metaverse Environment via Language.</em><br/>A. Abdari, <b>A. Falcon</b>, G. Serra.<br/><b>MMIR@ACM MM 2023.</b></p>
    <p><b>TL;DR</b>: </p>
    </td>
  </tr>
</table>



<h3>Video Question Answering</h3>
<h5><em>topics: multimedia, cross-modal understanding, vision and language</em></h5>

<h6>Selected publications/endeavors</h6>

<table border=none>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/mtap_vqa.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>Video question answering supported by a multi-task learning objective.</em><br/><b>A. Falcon</b>, G. Serra, O. Lanz.<br/><b>Multimedia Tools and Applications 82 (25), 38799-38826. 2023.</b></p>
    <p><b>TL;DR</b>: .</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/epic_eccv.png" alt="Overview of the algorithm" width="50%" height="auto">
    </td>
    <td width=50%>
      <p><em>Data augmentation techniques for the video question answering task.</em><br/><b>A. Falcon</b>, G. Serra, O. Lanz.<br/><b>EPIC@ECCV 2020.</b></p>
    <p><b>TL;DR</b>: </p>
    </td>
  </tr>
</table>



<h3>Remaining Useful Life Estimation</h3>
<h5><em>topics: predictive maintenance</em></h5>

<h6>Selected publications/endeavors</h6>

<table border=none>
 <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/compind.jpg" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>Neural turing machines for the remaining useful life estimation problem.</em><br/><b>A. Falcon</b>, G. D’Agostino, O. Lanz, G. Brajnik, C. Tasso, G. Serra.<br/><b>Computers in Industry 143, 103762. 2022.</b></p>
    <p><b>TL;DR</b>: .</p>
    </td>
  </tr>
 <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/aiabi.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>Estimating the Remaining Useful Life via Neural Sequence Models: a Comparative Study.</em><br/>G. D'Agostino, <b>A. Falcon</b>, O. Lanz, G. Brajnik, C. Tasso, G. Serra.<br/><b>AIABI@AIxIA 2022.</b></p>
    <p><b>TL;DR</b>: .</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/phme.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>A Dual-Stream architecture based on Neural Turing Machine and Attention for the Remaining Useful Life Estimation problem.</em><br/><b>A. Falcon</b>, G. D'Agostino, G. Serra, G. Brajnik, C. Tasso.<br/><b>PHME 2020.</b></p>
    <p><b>TL;DR</b>: .</p>
    </td>
  </tr>
  <tr>
    <td width=50%>
      <img src="https://raw.githubusercontent.com/aranciokov/aranciokov.github.io/master/assets/imgs/icphm.png" alt="Overview of the algorithm" width="100%" height="auto">
    </td>
    <td width=50%>
      <p><em>A neural turing machine-based approach to remaining useful life estimation.</em><br/><b>A. Falcon</b>, G. D'Agostino, G. Serra, G. Brajnik, C. Tasso.<br/><b>ICPHM 2020.</b></p>
    <p><b>TL;DR</b>: .</p>
    </td>
  </tr>
</table>




**Service**
* _Proceedings Chair_: [IRCDL 2023](https://ceur-ws.org/Vol-3365/)
* _Local Organization Chair_: [EQAI 2024, 2023](http://eqai.eu/), [ICIAP 2023](https://iciap2023.org/), [AIxIA 2022](https://aixia2022.uniud.it/)
* _Organizer_: [VIQA 2020](https://sites.google.com/view/viqa2020)/[VTIUR 2020](https://sites.google.com/view/vtiur2020)
* _Journal Reviewing_: IJCV, IEEE TMM, IET Computer Vision, ACM TOMM, IEEE Trans Hum Mach Syst.
* _Conference Reviewing_: ECCV 2024, ACM MM 2024, ACM MM 2023, CCISP 2023, IRCDL 2023, ICIAP 2023, ICPR 2022, ICIAP 2021, EMNLP 2021, ICPR 2020.
* _Co-Supervision_: 3 Bachelor and 4 Master students of Computer Science Degree or IoT, Big Data, and ML Degree at UniUD on topics related to Video&Language and Predictive Maintenance.
